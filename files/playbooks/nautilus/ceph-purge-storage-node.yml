---
# This playbook purges a single storage node
#
# It removes: containers, configuration files and ALL THE DATA
#
# The individual OSDs must first be removed from the Ceph cluster.
# The services must be stopped.

- name: confirm whether user really meant to purge the node

  hosts: localhost

  gather_facts: false

  vars_prompt:
    - name: ireallymeanit
      prompt: >
        Are you sure you want to purge the cluster?
        Note that if with_pkg is not set docker packages
        and more will be uninstalled from non-atomic hosts.
        Do you want to continue?
      default: 'no'
      private: no

  tasks:

  - name: exit playbook, if list of hosts contains more than one node
    fail:
      msg: >
        "Exiting purge-storage-node playbook, node was NOT purged.
        This playbook can only be used with a single node."
      when: play_hosts|length > 0

  - name: exit playbook, if user did not mean to purge node
    fail:
      msg: >
        "Exiting purge-storage-node playbook, node was NOT purged.
         To purge the node, either say 'yes' on the prompt or
         or use `-e ireallymeanit=yes` on the command line when
         invoking the playbook"
    when: ireallymeanit != 'yes'

  - name: set ceph_docker_registry value if not set
    set_fact:
      ceph_docker_registry: "docker.io"
    when: ceph_docker_registry is not defined

- name: purge ceph osd node

  hosts: "{{ osd_group_name | default('osds') }}"

  gather_facts: true
  become: true

  tasks:

  - name: exit playbook, if list of hosts contains more than one node
    fail:
      msg: >
        "Exiting purge-storage-node playbook, node was NOT purged.
        This playbook can only be used with a single node."
      when: play_hosts|length > 0

  - import_role:
      name: ceph-defaults

  - name: get all the running osds
    shell: |
      systemctl list-units --all | grep -oE "ceph-osd@([0-9]+).service"
    register: osd_units
    ignore_errors: true

  - name: disable ceph osd service
    service:
      name: "{{ item }}"
      state: stopped
      enabled: no
    with_items: "{{ osd_units.stdout_lines }}"

  - name: remove osd mountpoint tree
    file:
      path: /var/lib/ceph/osd/
      state: absent
    ignore_errors: true

  - name: default lvm_volumes if not defined
    set_fact:
      lvm_volumes: []
    when: lvm_volumes is not defined

  - name: zap and destroy osds created by ceph-volume with lvm_volumes
    ceph_volume:
      data: "{{ item.data }}"
      data_vg: "{{ item.data_vg|default(omit) }}"
      journal: "{{ item.journal|default(omit) }}"
      journal_vg: "{{ item.journal_vg|default(omit) }}"
      db: "{{ item.db|default(omit) }}"
      db_vg: "{{ item.db_vg|default(omit) }}"
      wal: "{{ item.wal|default(omit) }}"
      wal_vg: "{{ item.wal_vg|default(omit) }}"
      action: "zap"
    environment:
      CEPH_VOLUME_DEBUG: 1
      CEPH_CONTAINER_IMAGE: "{{ ceph_docker_registry }}/{{ ceph_docker_image }}:{{ ceph_docker_image_tag }}"
      CEPH_CONTAINER_BINARY: "{{ container_binary }}"
    with_items: "{{ lvm_volumes }}"
    when: lvm_volumes | default([]) | length > 0

  - name: zap and destroy osds created by ceph-volume with devices
    ceph_volume:
      data: "{{ item }}"
      action: "zap"
    environment:
      CEPH_VOLUME_DEBUG: 1
      CEPH_CONTAINER_IMAGE: "{{ ceph_docker_registry }}/{{ ceph_docker_image }}:{{ ceph_docker_image_tag }}"
      CEPH_CONTAINER_BINARY: "{{ container_binary }}"
    with_items: "{{ devices | default([]) }}"
    when: devices | default([]) | length > 0

  - name: remove ceph osd service
    file:
      path: /etc/systemd/system/ceph-osd@.service
      state: absent

  - name: include vars from group_vars/osds.yml
    include_vars:
      file: "{{ item }}"
    with_first_found:
      - files:
        - "{{ playbook_dir }}/group_vars/osds"
        - "{{ playbook_dir }}/group_vars/osds.yml"
        skip: true

  - name: find all osd_disk_prepare logs
    find:
      paths: "{{ ceph_osd_docker_run_script_path | default('/usr/share') }}"
      pattern: "ceph-osd-prepare-*.log"
    register: osd_disk_prepare_logs

  - name: ensure all osd_disk_prepare logs are removed
    file:
      path: "{{ item.path }}"
      state: absent
    with_items: "{{ osd_disk_prepare_logs.files }}"


- name: purge ceph directories

  hosts: "{{ osd_group_name | default('osds') }}"

  gather_facts: false # Already gathered previously
  become: true

  tasks:

  - name: exit playbook, if list of hosts contains more than one node
    fail:
      msg: >
        "Exiting purge-storage-node playbook, node was NOT purged.
        This playbook can only be used with a single node."
      when: play_hosts|length > 0

  - name: purge ceph directories for "{{ ansible_hostname }}"
    file:
      path: "{{ item }}"
      state: absent
    with_items:
      - /etc/ceph
      - /var/log/ceph

  - name: remove ceph data
    shell: rm -rf /var/lib/ceph/*
